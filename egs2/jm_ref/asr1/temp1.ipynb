{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('speech': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "611231d50d89399c7812e642eac108257e44491cbc2ead4a7a526b1be52f90d6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile\n",
    "import torch\n",
    "from espnet_model_zoo.downloader import ModelDownloader\n",
    "from espnet2.bin.asr_inference import Speech2Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_info(tensor):\n",
    "  info = []\n",
    "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
    "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
    "  info.append(f'tensor({str(tensor)})')\n",
    "  return ' '.join(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ESPnetASRModel(\n  (frontend): DefaultFrontend(\n    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)\n    (frontend): Frontend()\n    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)\n  )\n  (specaug): SpecAug(\n    (time_warp): TimeWarp(window=5, mode=bicubic)\n    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)\n    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)\n  )\n  (normalize): GlobalMVN(stats_file=/home/jmpark/.conda/envs/speech/lib/python3.8/site-packages/espnet_model_zoo/653d10049fdc264f694f57b49849343e/exp/asr_stats_raw_sp/train/feats_stats.npz, norm_means=True, norm_vars=True)\n  (encoder): TransformerEncoder(\n    (embed): Conv2dSubsampling6(\n      (conv): Sequential(\n        (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(2, 2))\n        (1): ReLU()\n        (2): Conv2d(512, 512, kernel_size=(5, 5), stride=(3, 3))\n        (3): ReLU()\n      )\n      (out): Sequential(\n        (0): Linear(in_features=6144, out_features=512, bias=True)\n        (1): PositionalEncoding(\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (encoders): MultiSequential(\n      (0): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (1): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (2): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (3): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (4): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (5): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (6): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (7): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (8): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (9): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (10): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (11): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (12): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (13): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (14): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (15): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (16): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (17): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n  )\n  (decoder): TransformerDecoder(\n    (embed): Sequential(\n      (0): Embedding(5000, 512)\n      (1): PositionalEncoding(\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n    (output_layer): Linear(in_features=512, out_features=5000, bias=True)\n    (decoders): MultiSequential(\n      (0): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (1): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (2): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (3): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (4): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (5): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (ctc): CTC(\n    (ctc_lo): Linear(in_features=512, out_features=5000, bias=True)\n    (ctc_loss): CTCLoss()\n  )\n  (criterion_att): LabelSmoothingLoss(\n    (criterion): KLDivLoss()\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "# Set test wav for attention image extraction\n",
    "TEST_DATA_PATH = \"./data/dev_clean\"\n",
    "WAV_LIST_PATH = TEST_DATA_PATH + \"/wav.scp\"\n",
    "ANSWER_LIST_PATH = TEST_DATA_PATH + \"/text\"\n",
    "\n",
    "file_name_list = []\n",
    "speech_ans_list = []\n",
    "\n",
    "with open(WAV_LIST_PATH, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        num, name = line.split(' ')\n",
    "        file_name_list.append(name[:-1])\n",
    "\n",
    "with open(ANSWER_LIST_PATH, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        speech_ans_list.append(line[17:])\n",
    "\n",
    "# Prepare model\n",
    "d = ModelDownloader()\n",
    "\n",
    "speech2text = Speech2Text(\n",
    "    **d.download_and_unpack('Shinji Watanabe/librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best'),\n",
    "    # Decoding parameters are not included in the model file\n",
    "    maxlenratio=0.0,\n",
    "    minlenratio=0.0,\n",
    "    beam_size=1,\n",
    "    ctc_weight=1.0,\n",
    "    lm_weight=0.0,\n",
    "    penalty=0.0,\n",
    "    nbest=1\n",
    ")\n",
    "# Add register hook for in encoder layers.\n",
    "net = speech2text.asr_model\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_num = 2 # selelct one of the wav in file_name_list\n",
    "speech, rate = soundfile.read(file_name_list[audio_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out, ctc_out = speech2text(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 4990,\n           0,    0,    0, 4784,    0,    0, 4997,    0,    0,    0,    0, 4875,\n           0,    0,    0,    0,    0, 4989,    0,    0,    0, 4965, 4965,    0,\n           0,    0, 4952,    0,    0,    0,    0, 4409, 4409, 4409, 4997,    0,\n           0, 3860,    0,    0,    0,    0,    0,    0,    0,    0, 3020,    0,\n           0,    0,    0, 4995,    0,    0,    0, 4998,    0,    0, 4458,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0, 4976,    0,    0,    0,    0,    0,    0, 1927,\n           0,    0,    0,    0,    0,    0,    0,    0,    0, 4996,    0,    0,\n        4601, 4601,    0, 4825, 4988, 4988,    0, 4971, 4971, 4987, 4987, 4894,\n        4894,    0,    0, 4597, 4597, 4958, 4953, 4980, 4980,    0,    0,    0,\n           0,    0,    0, 4880,    0,    0,    0,    0, 4875,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0, 2882,    0, 4161, 4161,    0,\n           0,    0,    0, 4997,    0,    0, 3429,    0,    0,    0,    0,    0,\n        4950,    0,    0,    0,    0, 4055,    0,    0, 4980, 4980,    0,    0,\n           0, 4996,    0,    0, 4879,    0,    0,    0,    0, 3946,    0,    0,\n           0,    0,    0, 4997,    0,    0,    0,    0,    0,    0,    0, 1120,\n           0,    0,    0,    0,    0,    0, 4819,    0,    0,    0,    0, 1050,\n           0,    0,    0,    0,    0,    0,    0,    0, 4994,    0,    0, 4998,\n           0,    0, 4766,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0])\n[('HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND', ['▁HE', '▁TELL', 'S', '▁US', '▁THAT', '▁AT', '▁THIS', '▁FE', 'S', 'TIVE', '▁SEASON', '▁OF', '▁THE', '▁YEAR', '▁WITH', '▁CHRISTMAS', '▁AND', '▁RO', 'AS', 'T', '▁BE', 'E', 'F', '▁LO', 'O', 'M', 'ING', '▁BEFORE', '▁US', '▁SIM', 'ILE', 'S', '▁DRAWN', '▁FROM', '▁EAT', 'ING', '▁AND', '▁ITS', '▁RESULT', 'S', '▁OCCUR', '▁MOST', '▁READILY', '▁TO', '▁THE', '▁MIND'], [4990, 4784, 4997, 4875, 4989, 4965, 4952, 4409, 4997, 3860, 3020, 4995, 4998, 4458, 4976, 1927, 4996, 4601, 4825, 4988, 4971, 4987, 4894, 4597, 4958, 4953, 4980, 4880, 4875, 2882, 4161, 4997, 3429, 4950, 4055, 4980, 4996, 4879, 3946, 4997, 1120, 4819, 1050, 4994, 4998, 4766], Hypothesis(yseq=tensor([4999, 4990, 4784, 4997, 4875, 4989, 4965, 4952, 4409, 4997, 3860, 3020,\n        4995, 4998, 4458, 4976, 1927, 4996, 4601, 4825, 4988, 4971, 4987, 4894,\n        4597, 4958, 4953, 4980, 4880, 4875, 2882, 4161, 4997, 3429, 4950, 4055,\n        4980, 4996, 4879, 3946, 4997, 1120, 4819, 1050, 4994, 4998, 4766, 4999]), score=tensor(-1.0052), scores={'ctc': tensor(-1.0052)}, states={'ctc': (tensor([[-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.0000e+10, -1.0000e+10],\n        [-1.4772e+03, -1.0000e+10],\n        [-1.4793e+03, -1.4772e+03],\n        [-1.4743e+03, -1.4773e+03],\n        [-1.4756e+03, -1.4894e+03],\n        [-1.4660e+03, -1.4761e+03],\n        [-1.4482e+03, -1.4660e+03],\n        [-1.4452e+03, -1.4482e+03],\n        [-1.4384e+03, -1.4452e+03],\n        [-1.4312e+03, -1.4384e+03],\n        [-1.4274e+03, -1.4312e+03],\n        [-1.4271e+03, -1.4274e+03],\n        [-1.3977e+03, -1.4265e+03],\n        [-1.3885e+03, -1.4154e+03],\n        [-1.3759e+03, -1.3885e+03],\n        [-1.3737e+03, -1.3759e+03],\n        [-1.3691e+03, -1.3736e+03],\n        [-1.3573e+03, -1.3691e+03],\n        [-1.3426e+03, -1.3730e+03],\n        [-1.3290e+03, -1.3428e+03],\n        [-1.3235e+03, -1.3290e+03],\n        [-1.3120e+03, -1.3235e+03],\n        [-1.2769e+03, -1.3235e+03],\n        [-1.2631e+03, -1.2769e+03],\n        [-1.2486e+03, -1.2631e+03],\n        [-1.2220e+03, -1.2660e+03],\n        [-1.2083e+03, -1.2220e+03],\n        [-1.2106e+03, -1.2083e+03],\n        [-1.2075e+03, -1.2082e+03],\n        [-1.1994e+03, -1.2071e+03],\n        [-1.1902e+03, -1.1994e+03],\n        [-1.1811e+03, -1.1902e+03],\n        [-1.1697e+03, -1.1811e+03],\n        [-1.1525e+03, -1.1697e+03],\n        [-1.1409e+03, -1.1525e+03],\n        [-1.1331e+03, -1.1409e+03],\n        [-1.1451e+03, -1.1331e+03],\n        [-1.1481e+03, -1.1331e+03],\n        [-1.1466e+03, -1.1331e+03],\n        [-1.1477e+03, -1.1331e+03],\n        [-1.1447e+03, -1.1331e+03],\n        [-1.1425e+03, -1.1331e+03],\n        [-1.1325e+03, -1.1331e+03],\n        [-1.1203e+03, -1.1472e+03],\n        [-1.1058e+03, -1.1203e+03],\n        [-1.0876e+03, -1.1058e+03],\n        [-1.0706e+03, -1.0876e+03],\n        [-1.0508e+03, -1.0706e+03],\n        [-1.0319e+03, -1.0508e+03],\n        [-1.0231e+03, -1.0319e+03],\n        [-1.0229e+03, -1.0330e+03],\n        [-1.0238e+03, -1.0229e+03],\n        [-1.0225e+03, -1.0225e+03],\n        [-1.0221e+03, -1.0219e+03],\n        [-1.0014e+03, -1.0213e+03],\n        [-9.8631e+02, -1.0014e+03],\n        [-9.7785e+02, -9.8653e+02],\n        [-9.6576e+02, -9.7788e+02],\n        [-9.6904e+02, -9.6577e+02],\n        [-9.6757e+02, -9.6575e+02],\n        [-9.6268e+02, -9.7198e+02],\n        [-9.5477e+02, -9.6269e+02],\n        [-9.1537e+02, -9.5477e+02],\n        [-9.0646e+02, -9.2165e+02],\n        [-8.9628e+02, -9.1561e+02],\n        [-8.6489e+02, -8.9629e+02],\n        [-8.6148e+02, -8.8285e+02],\n        [-8.7166e+02, -8.7005e+02],\n        [-8.7083e+02, -8.7673e+02],\n        [-8.6601e+02, -8.7086e+02],\n        [-8.5928e+02, -8.7294e+02],\n        [-8.4956e+02, -8.8321e+02],\n        [-8.4767e+02, -8.5854e+02],\n        [-8.5693e+02, -8.6174e+02],\n        [-8.6328e+02, -8.6962e+02],\n        [-8.7379e+02, -8.7356e+02],\n        [-8.7403e+02, -8.7298e+02],\n        [-8.7400e+02, -8.7289e+02],\n        [-8.8154e+02, -8.8513e+02],\n        [-8.8721e+02, -8.9619e+02],\n        [-8.9215e+02, -8.9374e+02],\n        [-8.9464e+02, -8.9950e+02],\n        [-8.8965e+02, -9.0469e+02],\n        [-8.9216e+02, -8.9130e+02],\n        [-8.7176e+02, -8.9095e+02],\n        [-8.7415e+02, -8.7176e+02],\n        [-8.7189e+02, -8.7167e+02],\n        [-8.7073e+02, -8.7108e+02],\n        [-8.7654e+02, -8.7019e+02],\n        [-8.7079e+02, -8.7019e+02],\n        [-8.6410e+02, -8.8216e+02],\n        [-8.7679e+02, -8.6410e+02],\n        [-8.6253e+02, -8.6410e+02],\n        [-8.5953e+02, -8.6234e+02],\n        [-8.4364e+02, -8.5947e+02],\n        [-8.0998e+02, -8.5943e+02],\n        [-8.1077e+02, -8.0998e+02],\n        [-8.1003e+02, -8.0961e+02],\n        [-7.8117e+02, -8.0910e+02],\n        [-7.3613e+02, -7.8117e+02],\n        [-6.8603e+02, -7.3613e+02],\n        [-6.4741e+02, -6.8603e+02],\n        [-6.1982e+02, -6.4741e+02],\n        [-5.8953e+02, -6.1982e+02],\n        [-5.8237e+02, -5.8953e+02],\n        [-5.7568e+02, -5.8237e+02],\n        [-5.6128e+02, -5.7568e+02],\n        [-5.5911e+02, -5.6128e+02],\n        [-5.5227e+02, -5.5900e+02],\n        [-5.4282e+02, -5.5227e+02],\n        [-5.4364e+02, -5.4282e+02],\n        [-5.4897e+02, -5.4245e+02],\n        [-5.4269e+02, -5.4245e+02],\n        [-5.4049e+02, -5.4187e+02],\n        [-5.3838e+02, -5.4027e+02],\n        [-5.3689e+02, -5.3824e+02],\n        [-5.3266e+02, -5.3666e+02],\n        [-5.2447e+02, -5.3264e+02],\n        [-5.0954e+02, -5.3447e+02],\n        [-5.0511e+02, -5.0955e+02],\n        [-4.9233e+02, -5.0771e+02],\n        [-4.7986e+02, -5.0113e+02],\n        [-4.7347e+02, -4.7986e+02],\n        [-4.7166e+02, -4.7346e+02],\n        [-4.7311e+02, -4.7151e+02],\n        [-4.7201e+02, -4.7133e+02],\n        [-4.6648e+02, -4.7712e+02],\n        [-4.7006e+02, -4.6648e+02],\n        [-4.7859e+02, -4.6645e+02],\n        [-4.7394e+02, -4.8615e+02],\n        [-4.8205e+02, -4.7395e+02],\n        [-4.8400e+02, -4.7395e+02],\n        [-4.7990e+02, -4.7395e+02],\n        [-4.7529e+02, -4.7395e+02],\n        [-4.7306e+02, -4.7371e+02],\n        [-4.5711e+02, -4.8513e+02],\n        [-4.5267e+02, -4.5711e+02],\n        [-4.4425e+02, -4.5266e+02],\n        [-4.5174e+02, -4.4425e+02],\n        [-4.4241e+02, -4.4438e+02],\n        [-4.3083e+02, -4.5422e+02],\n        [-4.3485e+02, -4.3083e+02],\n        [-4.3961e+02, -4.3081e+02],\n        [-4.3187e+02, -4.3213e+02],\n        [-4.3226e+02, -4.4731e+02],\n        [-4.3300e+02, -4.3226e+02],\n        [-4.2930e+02, -4.3187e+02],\n        [-4.2892e+02, -4.2923e+02],\n        [-4.1579e+02, -4.3784e+02],\n        [-4.1170e+02, -4.1579e+02],\n        [-4.0390e+02, -4.1169e+02],\n        [-3.9652e+02, -4.1446e+02],\n        [-3.8695e+02, -3.9652e+02],\n        [-3.6603e+02, -3.8695e+02],\n        [-3.4821e+02, -3.6603e+02],\n        [-3.3419e+02, -3.4839e+02],\n        [-3.2684e+02, -3.5025e+02],\n        [-3.2569e+02, -3.2684e+02],\n        [-3.2384e+02, -3.2542e+02],\n        [-3.0538e+02, -3.2365e+02],\n        [-3.0360e+02, -3.0538e+02],\n        [-2.9436e+02, -3.0347e+02],\n        [-2.8783e+02, -3.0383e+02],\n        [-2.7131e+02, -2.8786e+02],\n        [-2.5710e+02, -2.7131e+02],\n        [-2.4085e+02, -2.5710e+02],\n        [-2.2510e+02, -2.4085e+02],\n        [-2.1065e+02, -2.2510e+02],\n        [-2.2122e+02, -2.1065e+02],\n        [-2.1633e+02, -2.1068e+02],\n        [-2.1430e+02, -2.2879e+02],\n        [-2.2148e+02, -2.1430e+02],\n        [-2.1750e+02, -2.1430e+02],\n        [-2.1548e+02, -2.1426e+02],\n        [-2.1990e+02, -2.1400e+02],\n        [-1.9927e+02, -2.1400e+02],\n        [-1.8880e+02, -1.9927e+02],\n        [-1.7999e+02, -2.0442e+02],\n        [-1.8123e+02, -1.7999e+02],\n        [-1.7820e+02, -1.7973e+02],\n        [-1.8537e+02, -1.7800e+02],\n        [-1.8126e+02, -1.7805e+02],\n        [-1.6863e+02, -1.9458e+02],\n        [-1.6056e+02, -1.6865e+02],\n        [-1.5074e+02, -1.6056e+02],\n        [-1.3508e+02, -1.5074e+02],\n        [-1.2259e+02, -1.3508e+02],\n        [-1.1353e+02, -1.2259e+02],\n        [-1.1010e+02, -1.1353e+02],\n        [-1.1199e+02, -1.1007e+02],\n        [-1.1462e+02, -1.0993e+02],\n        [-1.0662e+02, -1.2288e+02],\n        [-1.0663e+02, -1.0663e+02],\n        [-1.0221e+02, -1.0594e+02],\n        [-8.6601e+01, -1.1025e+02],\n        [-7.3174e+01, -8.6601e+01],\n        [-7.8952e+01, -7.3174e+01],\n        [-5.1315e+01, -8.8912e+01],\n        [-4.9585e+01, -5.1315e+01],\n        [-5.3085e+01, -4.9422e+01],\n        [-4.5501e+01, -4.9397e+01],\n        [-4.0279e+01, -4.5481e+01],\n        [-3.8101e+01, -4.0274e+01],\n        [-3.4676e+01, -3.7993e+01],\n        [-3.3020e+01, -3.4640e+01],\n        [-3.2431e+01, -3.2839e+01],\n        [-3.2763e+01, -3.1922e+01],\n        [-3.2613e+01, -3.1563e+01],\n        [-3.5841e+01, -3.1263e+01],\n        [-3.3315e+01, -3.1253e+01],\n        [-3.5426e+01, -3.1133e+01],\n        [-3.5578e+01, -3.1120e+01],\n        [-3.6124e+01, -3.1108e+01],\n        [-3.3973e+01, -3.1101e+01]]), tensor([-1.0052, -1.0052, -1.0052,  ..., -1.0052, -1.0052, -1.0052]), 0, 0)}))]\n"
     ]
    }
   ],
   "source": [
    "print(ctc_out.argmax(dim=1))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hyp in out:\n",
    "    print(hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out[0][0])\n",
    "print(speech_ans_list[audio_num])\n",
    "print(out[0][2])\n",
    "\n",
    "# print(out[0][3])\n",
    "print(out[0][3][3]['ctc'][0].shape)\n",
    "print(out[0][3][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dh(model, layer_idx, head_idx, module_type, attn_type):\n",
    "    # module_type : 'encoder', 'decoder'\n",
    "    # attn_type : 'self_attn', 'src_attn'\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if f'{module_type}.{module_type}s.{layer_idx}.{attn_type}.linear_q' == name:\n",
    "            print(name)\n",
    "            print(module)\n",
    "            # print(param.shape)\n",
    "            return name, module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, module = apply_dh(net, 0, 0, 'encoder', 'self_attn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(module.weight.shape)\n",
    "print(module.weight.view(-1, 8, 64).shape)\n",
    "print(module.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for head in range(8):\n",
    "    module.bias.view(-1, 8, 64)[:, head, :] = 0\n",
    "    if head != 0:\n",
    "        module.weight.transpose(0, 1).view(-1, 8, 64)[:, head, :] = 0\n",
    "\n",
    "print(module.weight)\n",
    "print(module.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_input = torch.rand(1, 99, 512)\n",
    "out = module(temp_input).view(1, -1, 8, 64).transpose(1, 2)\n",
    "print(out.shape)\n",
    "print(out[0, 1, :])\n"
   ]
  }
 ]
}