{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('speech': conda)",
   "metadata": {
    "interpreter": {
     "hash": "611231d50d89399c7812e642eac108257e44491cbc2ead4a7a526b1be52f90d6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from espnet_model_zoo.downloader import ModelDownloader\n",
    "from espnet2.bin.asr_inference import Speech2Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_info(tensor):\n",
    "  info = []\n",
    "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
    "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
    "  return ' '.join(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ESPnetASRModel(\n  (frontend): DefaultFrontend(\n    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)\n    (frontend): Frontend()\n    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)\n  )\n  (specaug): SpecAug(\n    (time_warp): TimeWarp(window=5, mode=bicubic)\n    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)\n    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)\n  )\n  (normalize): GlobalMVN(stats_file=/home/jmpark/.conda/envs/speech/lib/python3.8/site-packages/espnet_model_zoo/653d10049fdc264f694f57b49849343e/exp/asr_stats_raw_sp/train/feats_stats.npz, norm_means=True, norm_vars=True)\n  (encoder): TransformerEncoder(\n    (embed): Conv2dSubsampling6(\n      (conv): Sequential(\n        (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(2, 2))\n        (1): ReLU()\n        (2): Conv2d(512, 512, kernel_size=(5, 5), stride=(3, 3))\n        (3): ReLU()\n      )\n      (out): Sequential(\n        (0): Linear(in_features=6144, out_features=512, bias=True)\n        (1): PositionalEncoding(\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (encoders): MultiSequential(\n      (0): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (1): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (2): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (3): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (4): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (5): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (6): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (7): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (8): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (9): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (10): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (11): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (12): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (13): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (14): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (15): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (16): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (17): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n  )\n  (decoder): TransformerDecoder(\n    (embed): Sequential(\n      (0): Embedding(5000, 512)\n      (1): PositionalEncoding(\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n    (output_layer): Linear(in_features=512, out_features=5000, bias=True)\n    (decoders): MultiSequential(\n      (0): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (1): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (2): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (3): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (4): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (5): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (ctc): CTC(\n    (ctc_lo): Linear(in_features=512, out_features=5000, bias=True)\n    (ctc_loss): CTCLoss()\n  )\n  (criterion_att): LabelSmoothingLoss(\n    (criterion): KLDivLoss()\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "# Set test wav for attention image extraction\n",
    "TEST_DATA_PATH = \"./../data/dev_clean\"\n",
    "WAV_LIST_PATH = TEST_DATA_PATH + \"/wav.scp\"\n",
    "ANSWER_LIST_PATH = TEST_DATA_PATH + \"/text\"\n",
    "\n",
    "file_name_list = []\n",
    "speech_ans_list = []\n",
    "\n",
    "with open(WAV_LIST_PATH, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        num, name = line.split(' ')\n",
    "        file_name_list.append(name[:-1])\n",
    "\n",
    "with open(ANSWER_LIST_PATH, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        speech_ans_list.append(line[17:])\n",
    "\n",
    "# Prepare model\n",
    "d = ModelDownloader()\n",
    "\n",
    "speech2text = Speech2Text(\n",
    "    **d.download_and_unpack('Shinji Watanabe/librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best'),\n",
    "    # Decoding parameters are not included in the model file\n",
    "    maxlenratio=0.0,\n",
    "    minlenratio=0.0,\n",
    "    beam_size=1,\n",
    "    ctc_weight=1.0,\n",
    "    lm_weight=0.0,\n",
    "    penalty=0.0,\n",
    "    nbest=1,\n",
    "    out_mode=\"ctc\"\n",
    ")\n",
    "# Add register hook for in encoder layers.\n",
    "net = speech2text.asr_model\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_num = 20 # selelct one of the wav in file_name_list\n",
    "speech, rate = soundfile.read(file_name_list[audio_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hook():\n",
    "    def __init__(self, module):\n",
    "        self.hook_f = module.register_forward_hook(self.hook_f_fn)\n",
    "        self.target_output = None\n",
    "\n",
    "    def hook_f_fn(self, module, input, output):\n",
    "        self.target_output = input[0]\n",
    "\n",
    "    def close(self):\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_hook(model, layer_idx, module_type, attn_type):\n",
    "    # module_type : 'encoder', 'decoder'\n",
    "    # attn_type : 'self_attn', 'src_attn'\n",
    "    for name, module in model.named_modules():\n",
    "        if f'{module_type}.{module_type}s.{layer_idx}.{attn_type}.linear_out' == name:\n",
    "            hook = Hook(module=module)\n",
    "    \n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Hook' object has no attribute 'hook_b_fn'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a9149665f340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhook_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoder'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'self_attn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhook_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoder'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'self_attn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-fda0d16d9d4c>\u001b[0m in \u001b[0;36mapply_hook\u001b[0;34m(model, layer_idx, module_type, attn_type)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34mf'{module_type}.{module_type}s.{layer_idx}.{attn_type}.linear_out'\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-6aac58b3c8d9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook_f_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook_b_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Hook' object has no attribute 'hook_b_fn'"
     ]
    }
   ],
   "source": [
    "hook_0 = apply_hook(net, 0, 'encoder', 'self_attn')\n",
    "hook_1 = apply_hook(net, 1, 'encoder', 'self_attn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, ctc_out = speech2text(speech)\n",
    "ctc_argmax = ctc_out.argmax(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_idx = 50\n",
    "# grad_temp = torch.ones_like(ctc_out)\n",
    "# print(grad_temp.shape)\n",
    "# grad_temp[0, check_idx, ctc_argmax[0,check_idx]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = torch.zeros_like(ctc_out)\n",
    "one_hot.scatter_(2, ctc_argmax.unsqueeze(2), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_out.backward(gradient=one_hot, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if name == 'encoder.encoders.0.self_attn.linear_out.weight':\n",
    "        temp_weight = param\n",
    "\n",
    "    elif name == 'encoder.encoders.0.self_attn.linear_out.bias':\n",
    "        temp_bias = param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp_weight.shape)\n",
    "print(temp_bias.shape)"
   ]
  }
 ]
}