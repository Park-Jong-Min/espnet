{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "speech",
   "display_name": "speech",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_grad_cam_image import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hook():\n",
    "    def __init__(self, module):\n",
    "        self.hook_f = module.register_forward_hook(self.hook_f_fn)\n",
    "        self.target_output = None\n",
    "\n",
    "    def hook_f_fn(self, module, input, output):\n",
    "        self.target_output = input[0]\n",
    "\n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "def apply_hook(model, layer_idx, module_type, attn_type):\n",
    "    # module_type : 'encoder', 'decoder'\n",
    "    # attn_type : 'self_attn', 'src_attn'\n",
    "    for name, module in model.named_modules():\n",
    "        if f'{module_type}.{module_type}s.{layer_idx}.{attn_type}.linear_out' == name:\n",
    "            hook = Hook(module=module)\n",
    "    return hook\n",
    "\n",
    "exp_dir = '/home/jmpark/home_data_jmpark/espnet/egs2/jm_ref/asr1/exp'\n",
    "\n",
    "saved_encoder_grad_cam_images = []\n",
    "\n",
    "TEST_DATA_PATH = \"./../data/dev_clean\"\n",
    "WAV_LIST_PATH = TEST_DATA_PATH + \"/wav.scp\"\n",
    "ANSWER_LIST_PATH = TEST_DATA_PATH + \"/text\"\n",
    "\n",
    "file_name_list = []\n",
    "speech_ans_list = []\n",
    "\n",
    "with open(WAV_LIST_PATH, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        num, name = line.split(' ')\n",
    "        file_name_list.append(name[:-1])\n",
    "\n",
    "with open(ANSWER_LIST_PATH, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        speech_ans_list.append(line[17:])\n",
    "\n",
    "# Prepare model\n",
    "d = ModelDownloader()\n",
    "\n",
    "speech2text = Speech2Text(\n",
    "    **d.download_and_unpack('Shinji Watanabe/librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best'),\n",
    "    # Decoding parameters are not included in the model file\n",
    "    maxlenratio=0.0,\n",
    "    minlenratio=0.0,\n",
    "    beam_size=1,\n",
    "    ctc_weight=1.0,\n",
    "    lm_weight=0.0,\n",
    "    penalty=0.0,\n",
    "    nbest=1,\n",
    "    out_mode=\"default\"\n",
    ")\n",
    "\n",
    "hook_list = []\n",
    "\n",
    "# Add register hook for in encoder layers.\n",
    "net = speech2text.asr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech, rate = soundfile.read(file_name_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'text' and 'text_lengths'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f7a717a16704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mctc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeech2text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeech\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home_data/jmpark/espnet/espnet2/bin/asr_inference.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, speech)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"speech\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspeech\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"speech_lengths\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m             \u001b[0mctc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_output_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masr_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masr_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/speech/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'text' and 'text_lengths'"
     ]
    }
   ],
   "source": [
    "speech2text(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(18):\n",
    "    hook_list.append(apply_hook(net, i, 'encoder', 'self_attn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "word_num_list = []\n",
    "\n",
    "for audio in tqdm(range(len(file_name_list))):\n",
    "    audio_num = audio # selelct one of the wav in file_name_list\n",
    "    speech, rate = soundfile.read(file_name_list[audio_num])\n",
    "\n",
    "    out, ctc_out = speech2text(speech)\n",
    "    ctc_argmax = ctc_out.argmax(2)\n",
    "\n",
    "    one_hot = torch.zeros_like(ctc_out)\n",
    "    one_hot.scatter_(2, ctc_argmax.unsqueeze(2), 1.0)\n",
    "    img = make_grad_cam_img_list(model=net, target_out=ctc_out, target_loss=one_hot, hook_list=hook_list)\n",
    "    img_list.append(img)\n",
    "    word_num_list.append(audio_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_img_np = np.array(img_list)\n",
    "total_img_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_img = np.mean(total_img_np, axis=0)\n",
    "print(mean_img[17, 4])\n",
    "print(mean_img[7, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_img = []\n",
    "save_img.append(mean_img)\n",
    "\n",
    "save_encoder_grad_image(image_list=save_img, target_list=['mean'], audio_num='mean',\n",
    "                                n_targets=1, PATH=exp_dir + f'/feature_images/encoder_grad_cam/sentence/mean/')"
   ]
  }
 ]
}