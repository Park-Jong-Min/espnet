{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "espnet_1.7.1",
   "display_name": "espnet_1.7.1",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from espnet_model_zoo.downloader import ModelDownloader\n",
    "from espnet2.bin.asr_inference import Speech2Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set test wav for attention image extraction\n",
    "TEST_DATA_PATH = \"./data/dev_clean\"\n",
    "WAV_LIST_PATH = TEST_DATA_PATH + \"/wav.scp\"\n",
    "\n",
    "file_name_list = []\n",
    "audio_num = 1 # selelct one of the wav in file_name_list\n",
    "\n",
    "with open(WAV_LIST_PATH, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        num, name = line.split(' ')\n",
    "        file_name_list.append(name[:-1])\n",
    "\n",
    "speech, rate = soundfile.read(file_name_list[audio_num])\n",
    "\n",
    "# Prepare model\n",
    "d = ModelDownloader()\n",
    "\n",
    "ASR_MODEL_PATH = \"/home_data/jmpark/espnet/tools/anaconda/envs/espnet_1.7/lib/python3.8/site-packages/espnet_model_zoo/653d10049fdc264f694f57b49849343e/exp/asr_train_asr_transformer_e18_raw_bpe_sp/54epoch.pth\"\n",
    "LM_MODEL_PATH = \"/home_data/jmpark/espnet/tools/anaconda/envs/espnet_1.7.1/lib/python3.8/site-packages/espnet_model_zoo/653d10049fdc264f694f57b49849343e/exp/lm_train_lm_adam_bpe/17epoch.pth\"\n",
    "\n",
    "speech2text = Speech2Text(\n",
    "    **d.download_and_unpack('Shinji Watanabe/librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best'),\n",
    "    # Decoding parameters are not included in the model file\n",
    "    maxlenratio=0.0,\n",
    "    minlenratio=0.0,\n",
    "    beam_size=1,\n",
    "    ctc_weight=0.4,\n",
    "    lm_weight=0.6,\n",
    "    penalty=0.0,\n",
    "    nbest=1\n",
    ")\n",
    "# Add register hook for in encoder layers.\n",
    "net = speech2text.asr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "encoder.encoders.0.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\nencoder.encoders.1.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\nencoder.encoders.2.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\nencoder.encoders.3.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\nencoder.encoders.4.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\nencoder.encoders.5.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\nencoder.encoders.6.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\nencoder.encoders.7.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\nencoder.encoders.8.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\nencoder.encoders.9.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\nencoder.encoders.10.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\nencoder.encoders.11.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\nencoder.encoders.12.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\nencoder.encoders.13.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\nencoder.encoders.14.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\nencoder.encoders.15.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\nencoder.encoders.16.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\nencoder.encoders.17.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\n"
     ]
    }
   ],
   "source": [
    "saved_encoder_self_attn_images = []\n",
    "\n",
    "def attn_encoder(self, input_tensor, output_tensor):\n",
    "    cols = output_tensor[1].size(1)\n",
    "\n",
    "    for i in range(cols):\n",
    "        img = output_tensor[1][0,i,:]\n",
    "        saved_encoder_self_attn_images.append(img)\n",
    "\n",
    "for name, parameter in net.named_modules():\n",
    "    for i in range(18): \n",
    "        if 'encoder.encoders.'+ str(i) +'.self_attn' == name:\n",
    "            print(name)\n",
    "            print(parameter)\n",
    "            parameter.register_forward_hook(attn_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "decoder.decoders.0.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\ndecoder.decoders.1.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\ndecoder.decoders.2.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\ndecoder.decoders.3.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\ndecoder.decoders.4.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\ndecoder.decoders.5.self_attn\nMultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\n"
     ]
    }
   ],
   "source": [
    "saved_decoder_self_attn_images = []\n",
    "\n",
    "def attn_decoder(self, input_tensor, output_tensor):\n",
    "    cols = output_tensor[1].size(1)\n",
    "\n",
    "    for i in range(cols):\n",
    "        img = output_tensor[1][0,i,:]\n",
    "        saved_decoder_self_attn_images.append(img)\n",
    "\n",
    "for name, parameter in net.named_modules():\n",
    "    for i in range(6): \n",
    "        if 'decoder.decoders.'+ str(i) +'.self_attn' == name:\n",
    "            print(name)\n",
    "            print(parameter)\n",
    "            parameter.register_forward_hook(attn_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "saved_encoder_self_attn_images = []\n",
    "saved_decoder_self_attn_images = []\n",
    "out = speech2text(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_masked_image(image_list, name, n_layers, n_heads, PATH):\n",
    "    fig_saved_dir = PATH\n",
    "    # input image list has word_num * n_layers * n_heads\n",
    "    word_num = int(len(image_list) / (n_layers * n_heads))\n",
    "\n",
    "    img = torch.zeros((n_layers, n_heads, word_num, word_num))\n",
    "\n",
    "    for word in range(word_num):\n",
    "        for layer in range(n_layers):\n",
    "            for head in range(n_heads):\n",
    "                # img_piece size is word + 1 \n",
    "                img_piece = image_list[word*(n_layers*n_heads)+layer*(n_heads)+head][0]\n",
    "                img[layer, head, word, :word+1] = img_piece\n",
    "\n",
    "    for layer in range(n_layers):\n",
    "        fig = plt.figure(figsize=(100,100))\n",
    "        axes = []\n",
    "        for head in range(n_heads):\n",
    "            img_save = img[layer][head]\n",
    "            axes.append(fig.add_subplot(1, n_heads, head+1))\n",
    "            plt.imshow(img_save)\n",
    "            plt.axis('off')\n",
    "        \n",
    "        print('process {0} layer images....'.format(layer))\n",
    "        plt.savefig(fig_saved_dir + 'audio' + str(audio_num) + '_' + name +'_layer{0}_attention.png'.format(layer),\n",
    "                    bbox_inches='tight',\n",
    "                    dpi=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "90\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 1, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "print(len(saved_decoder_self_attn_images))\n",
    "saved_decoder_self_attn_images[10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[8, 1, 1]}, size=[1]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (3)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3ec07b42e239>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_masked_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_decoder_self_attn_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'decoder_self_attn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./exp/feature_images/decoder_self_attn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-a2315e7e15be>\u001b[0m in \u001b[0;36msave_masked_image\u001b[0;34m(image_list, name, n_layers, n_heads, PATH)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0;31m# img_piece size is word + 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mimg_piece\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_piece\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[8, 1, 1]}, size=[1]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (3)"
     ]
    }
   ],
   "source": [
    "save_masked_image(saved_decoder_self_attn_images, 'decoder_self_attn', 6, 8, './exp/feature_images/decoder_self_attn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_unmasked_image(saved_encoder_self_attn_images, 'encoder_self_attn', 18, 8, './exp/feature_images/encoder_self_attn/')\n"
   ]
  }
 ]
}