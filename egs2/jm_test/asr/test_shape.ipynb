{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "espnet_1.7",
   "display_name": "espnet_1.7",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile\n",
    "import torch\n",
    "from espnet_model_zoo.downloader import ModelDownloader\n",
    "from espnet2.bin.asr_inference import Speech2Text\n",
    "from espnet2.asr.frontend.default import *\n",
    "from espnet.nets.pytorch_backend.transformer.subsampling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ModelDownloader()\n",
    "ASR_MODEL_PATH = \"/home/jmpark/espnet/tools/anaconda/envs/espnet_1.7/lib/python3.8/site-packages/espnet_model_zoo/653d10049fdc264f694f57b49849343e/exp/asr_train_asr_transformer_e18_raw_bpe_sp/54epoch.pth\"\n",
    "LM_MODEL_PATH = \"/home/jmpark/espnet/tools/anaconda/envs/espnet_1.7/lib/python3.8/site-packages/espnet_model_zoo/653d10049fdc264f694f57b49849343e/exp/lm_train_lm_adam_bpe/17epoch.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech2text = Speech2Text(\n",
    "    **d.download_and_unpack('Shinji Watanabe/librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best'),\n",
    "    # Decoding parameters are not included in the model file\n",
    "    maxlenratio=0.0,\n",
    "    minlenratio=0.0,\n",
    "    beam_size=1,\n",
    "    ctc_weight=0.4,\n",
    "    lm_weight=0.6,\n",
    "    penalty=0.0,\n",
    "    nbest=1\n",
    ")\n",
    "\n",
    "net = speech2text.asr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ESPnetASRModel(\n  (frontend): DefaultFrontend(\n    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)\n    (frontend): Frontend()\n    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)\n  )\n  (specaug): SpecAug(\n    (time_warp): TimeWarp(window=5, mode=bicubic)\n    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)\n    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)\n  )\n  (normalize): GlobalMVN(stats_file=/home/jmpark/espnet/tools/anaconda/envs/espnet_1.7/lib/python3.8/site-packages/espnet_model_zoo/653d10049fdc264f694f57b49849343e/exp/asr_stats_raw_sp/train/feats_stats.npz, norm_means=True, norm_vars=True)\n  (encoder): TransformerEncoder(\n    (embed): Conv2dSubsampling6(\n      (conv): Sequential(\n        (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(2, 2))\n        (1): ReLU()\n        (2): Conv2d(512, 512, kernel_size=(5, 5), stride=(3, 3))\n        (3): ReLU()\n      )\n      (out): Sequential(\n        (0): Linear(in_features=6144, out_features=512, bias=True)\n        (1): PositionalEncoding(\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (encoders): MultiSequential(\n      (0): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (1): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (2): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (3): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (4): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (5): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (6): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (7): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (8): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (9): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (10): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (11): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (12): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (13): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (14): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (15): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (16): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (17): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n  )\n  (decoder): TransformerDecoder(\n    (embed): Sequential(\n      (0): Embedding(5000, 512)\n      (1): PositionalEncoding(\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n    (output_layer): Linear(in_features=512, out_features=5000, bias=True)\n    (decoders): MultiSequential(\n      (0): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (1): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (2): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (3): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (4): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (5): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (ctc): CTC(\n    (ctc_lo): Linear(in_features=512, out_features=5000, bias=True)\n    (ctc_loss): CTCLoss()\n  )\n  (criterion_att): LabelSmoothingLoss(\n    (criterion): KLDivLoss()\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nfrontend\nfrontend.stft\nfrontend.frontend\nfrontend.logmel\nspecaug\nspecaug.time_warp\nspecaug.freq_mask\nspecaug.time_mask\nnormalize\nencoder\nencoder.embed\nencoder.embed.conv\nencoder.embed.conv.0\nencoder.embed.conv.1\nencoder.embed.conv.2\nencoder.embed.conv.3\nencoder.embed.out\nencoder.embed.out.0\nencoder.embed.out.1\nencoder.embed.out.1.dropout\nencoder.encoders\nencoder.encoders.0\nencoder.encoders.0.self_attn\nencoder.encoders.0.self_attn.linear_q\nencoder.encoders.0.self_attn.linear_k\nencoder.encoders.0.self_attn.linear_v\nencoder.encoders.0.self_attn.linear_out\nencoder.encoders.0.self_attn.dropout\nencoder.encoders.0.feed_forward\nencoder.encoders.0.feed_forward.w_1\nencoder.encoders.0.feed_forward.w_2\nencoder.encoders.0.feed_forward.dropout\nencoder.encoders.0.feed_forward.activation\nencoder.encoders.0.norm1\nencoder.encoders.0.norm2\nencoder.encoders.0.dropout\nencoder.encoders.1\nencoder.encoders.1.self_attn\nencoder.encoders.1.self_attn.linear_q\nencoder.encoders.1.self_attn.linear_k\nencoder.encoders.1.self_attn.linear_v\nencoder.encoders.1.self_attn.linear_out\nencoder.encoders.1.self_attn.dropout\nencoder.encoders.1.feed_forward\nencoder.encoders.1.feed_forward.w_1\nencoder.encoders.1.feed_forward.w_2\nencoder.encoders.1.feed_forward.dropout\nencoder.encoders.1.norm1\nencoder.encoders.1.norm2\nencoder.encoders.1.dropout\nencoder.encoders.2\nencoder.encoders.2.self_attn\nencoder.encoders.2.self_attn.linear_q\nencoder.encoders.2.self_attn.linear_k\nencoder.encoders.2.self_attn.linear_v\nencoder.encoders.2.self_attn.linear_out\nencoder.encoders.2.self_attn.dropout\nencoder.encoders.2.feed_forward\nencoder.encoders.2.feed_forward.w_1\nencoder.encoders.2.feed_forward.w_2\nencoder.encoders.2.feed_forward.dropout\nencoder.encoders.2.norm1\nencoder.encoders.2.norm2\nencoder.encoders.2.dropout\nencoder.encoders.3\nencoder.encoders.3.self_attn\nencoder.encoders.3.self_attn.linear_q\nencoder.encoders.3.self_attn.linear_k\nencoder.encoders.3.self_attn.linear_v\nencoder.encoders.3.self_attn.linear_out\nencoder.encoders.3.self_attn.dropout\nencoder.encoders.3.feed_forward\nencoder.encoders.3.feed_forward.w_1\nencoder.encoders.3.feed_forward.w_2\nencoder.encoders.3.feed_forward.dropout\nencoder.encoders.3.norm1\nencoder.encoders.3.norm2\nencoder.encoders.3.dropout\nencoder.encoders.4\nencoder.encoders.4.self_attn\nencoder.encoders.4.self_attn.linear_q\nencoder.encoders.4.self_attn.linear_k\nencoder.encoders.4.self_attn.linear_v\nencoder.encoders.4.self_attn.linear_out\nencoder.encoders.4.self_attn.dropout\nencoder.encoders.4.feed_forward\nencoder.encoders.4.feed_forward.w_1\nencoder.encoders.4.feed_forward.w_2\nencoder.encoders.4.feed_forward.dropout\nencoder.encoders.4.norm1\nencoder.encoders.4.norm2\nencoder.encoders.4.dropout\nencoder.encoders.5\nencoder.encoders.5.self_attn\nencoder.encoders.5.self_attn.linear_q\nencoder.encoders.5.self_attn.linear_k\nencoder.encoders.5.self_attn.linear_v\nencoder.encoders.5.self_attn.linear_out\nencoder.encoders.5.self_attn.dropout\nencoder.encoders.5.feed_forward\nencoder.encoders.5.feed_forward.w_1\nencoder.encoders.5.feed_forward.w_2\nencoder.encoders.5.feed_forward.dropout\nencoder.encoders.5.norm1\nencoder.encoders.5.norm2\nencoder.encoders.5.dropout\nencoder.encoders.6\nencoder.encoders.6.self_attn\nencoder.encoders.6.self_attn.linear_q\nencoder.encoders.6.self_attn.linear_k\nencoder.encoders.6.self_attn.linear_v\nencoder.encoders.6.self_attn.linear_out\nencoder.encoders.6.self_attn.dropout\nencoder.encoders.6.feed_forward\nencoder.encoders.6.feed_forward.w_1\nencoder.encoders.6.feed_forward.w_2\nencoder.encoders.6.feed_forward.dropout\nencoder.encoders.6.norm1\nencoder.encoders.6.norm2\nencoder.encoders.6.dropout\nencoder.encoders.7\nencoder.encoders.7.self_attn\nencoder.encoders.7.self_attn.linear_q\nencoder.encoders.7.self_attn.linear_k\nencoder.encoders.7.self_attn.linear_v\nencoder.encoders.7.self_attn.linear_out\nencoder.encoders.7.self_attn.dropout\nencoder.encoders.7.feed_forward\nencoder.encoders.7.feed_forward.w_1\nencoder.encoders.7.feed_forward.w_2\nencoder.encoders.7.feed_forward.dropout\nencoder.encoders.7.norm1\nencoder.encoders.7.norm2\nencoder.encoders.7.dropout\nencoder.encoders.8\nencoder.encoders.8.self_attn\nencoder.encoders.8.self_attn.linear_q\nencoder.encoders.8.self_attn.linear_k\nencoder.encoders.8.self_attn.linear_v\nencoder.encoders.8.self_attn.linear_out\nencoder.encoders.8.self_attn.dropout\nencoder.encoders.8.feed_forward\nencoder.encoders.8.feed_forward.w_1\nencoder.encoders.8.feed_forward.w_2\nencoder.encoders.8.feed_forward.dropout\nencoder.encoders.8.norm1\nencoder.encoders.8.norm2\nencoder.encoders.8.dropout\nencoder.encoders.9\nencoder.encoders.9.self_attn\nencoder.encoders.9.self_attn.linear_q\nencoder.encoders.9.self_attn.linear_k\nencoder.encoders.9.self_attn.linear_v\nencoder.encoders.9.self_attn.linear_out\nencoder.encoders.9.self_attn.dropout\nencoder.encoders.9.feed_forward\nencoder.encoders.9.feed_forward.w_1\nencoder.encoders.9.feed_forward.w_2\nencoder.encoders.9.feed_forward.dropout\nencoder.encoders.9.norm1\nencoder.encoders.9.norm2\nencoder.encoders.9.dropout\nencoder.encoders.10\nencoder.encoders.10.self_attn\nencoder.encoders.10.self_attn.linear_q\nencoder.encoders.10.self_attn.linear_k\nencoder.encoders.10.self_attn.linear_v\nencoder.encoders.10.self_attn.linear_out\nencoder.encoders.10.self_attn.dropout\nencoder.encoders.10.feed_forward\nencoder.encoders.10.feed_forward.w_1\nencoder.encoders.10.feed_forward.w_2\nencoder.encoders.10.feed_forward.dropout\nencoder.encoders.10.norm1\nencoder.encoders.10.norm2\nencoder.encoders.10.dropout\nencoder.encoders.11\nencoder.encoders.11.self_attn\nencoder.encoders.11.self_attn.linear_q\nencoder.encoders.11.self_attn.linear_k\nencoder.encoders.11.self_attn.linear_v\nencoder.encoders.11.self_attn.linear_out\nencoder.encoders.11.self_attn.dropout\nencoder.encoders.11.feed_forward\nencoder.encoders.11.feed_forward.w_1\nencoder.encoders.11.feed_forward.w_2\nencoder.encoders.11.feed_forward.dropout\nencoder.encoders.11.norm1\nencoder.encoders.11.norm2\nencoder.encoders.11.dropout\nencoder.encoders.12\nencoder.encoders.12.self_attn\nencoder.encoders.12.self_attn.linear_q\nencoder.encoders.12.self_attn.linear_k\nencoder.encoders.12.self_attn.linear_v\nencoder.encoders.12.self_attn.linear_out\nencoder.encoders.12.self_attn.dropout\nencoder.encoders.12.feed_forward\nencoder.encoders.12.feed_forward.w_1\nencoder.encoders.12.feed_forward.w_2\nencoder.encoders.12.feed_forward.dropout\nencoder.encoders.12.norm1\nencoder.encoders.12.norm2\nencoder.encoders.12.dropout\nencoder.encoders.13\nencoder.encoders.13.self_attn\nencoder.encoders.13.self_attn.linear_q\nencoder.encoders.13.self_attn.linear_k\nencoder.encoders.13.self_attn.linear_v\nencoder.encoders.13.self_attn.linear_out\nencoder.encoders.13.self_attn.dropout\nencoder.encoders.13.feed_forward\nencoder.encoders.13.feed_forward.w_1\nencoder.encoders.13.feed_forward.w_2\nencoder.encoders.13.feed_forward.dropout\nencoder.encoders.13.norm1\nencoder.encoders.13.norm2\nencoder.encoders.13.dropout\nencoder.encoders.14\nencoder.encoders.14.self_attn\nencoder.encoders.14.self_attn.linear_q\nencoder.encoders.14.self_attn.linear_k\nencoder.encoders.14.self_attn.linear_v\nencoder.encoders.14.self_attn.linear_out\nencoder.encoders.14.self_attn.dropout\nencoder.encoders.14.feed_forward\nencoder.encoders.14.feed_forward.w_1\nencoder.encoders.14.feed_forward.w_2\nencoder.encoders.14.feed_forward.dropout\nencoder.encoders.14.norm1\nencoder.encoders.14.norm2\nencoder.encoders.14.dropout\nencoder.encoders.15\nencoder.encoders.15.self_attn\nencoder.encoders.15.self_attn.linear_q\nencoder.encoders.15.self_attn.linear_k\nencoder.encoders.15.self_attn.linear_v\nencoder.encoders.15.self_attn.linear_out\nencoder.encoders.15.self_attn.dropout\nencoder.encoders.15.feed_forward\nencoder.encoders.15.feed_forward.w_1\nencoder.encoders.15.feed_forward.w_2\nencoder.encoders.15.feed_forward.dropout\nencoder.encoders.15.norm1\nencoder.encoders.15.norm2\nencoder.encoders.15.dropout\nencoder.encoders.16\nencoder.encoders.16.self_attn\nencoder.encoders.16.self_attn.linear_q\nencoder.encoders.16.self_attn.linear_k\nencoder.encoders.16.self_attn.linear_v\nencoder.encoders.16.self_attn.linear_out\nencoder.encoders.16.self_attn.dropout\nencoder.encoders.16.feed_forward\nencoder.encoders.16.feed_forward.w_1\nencoder.encoders.16.feed_forward.w_2\nencoder.encoders.16.feed_forward.dropout\nencoder.encoders.16.norm1\nencoder.encoders.16.norm2\nencoder.encoders.16.dropout\nencoder.encoders.17\nencoder.encoders.17.self_attn\nencoder.encoders.17.self_attn.linear_q\nencoder.encoders.17.self_attn.linear_k\nencoder.encoders.17.self_attn.linear_v\nencoder.encoders.17.self_attn.linear_out\nencoder.encoders.17.self_attn.dropout\nencoder.encoders.17.feed_forward\nencoder.encoders.17.feed_forward.w_1\nencoder.encoders.17.feed_forward.w_2\nencoder.encoders.17.feed_forward.dropout\nencoder.encoders.17.norm1\nencoder.encoders.17.norm2\nencoder.encoders.17.dropout\nencoder.after_norm\ndecoder\ndecoder.embed\ndecoder.embed.0\ndecoder.embed.1\ndecoder.embed.1.dropout\ndecoder.after_norm\ndecoder.output_layer\ndecoder.decoders\ndecoder.decoders.0\ndecoder.decoders.0.self_attn\ndecoder.decoders.0.self_attn.linear_q\ndecoder.decoders.0.self_attn.linear_k\ndecoder.decoders.0.self_attn.linear_v\ndecoder.decoders.0.self_attn.linear_out\ndecoder.decoders.0.self_attn.dropout\ndecoder.decoders.0.src_attn\ndecoder.decoders.0.src_attn.linear_q\ndecoder.decoders.0.src_attn.linear_k\ndecoder.decoders.0.src_attn.linear_v\ndecoder.decoders.0.src_attn.linear_out\ndecoder.decoders.0.src_attn.dropout\ndecoder.decoders.0.feed_forward\ndecoder.decoders.0.feed_forward.w_1\ndecoder.decoders.0.feed_forward.w_2\ndecoder.decoders.0.feed_forward.dropout\ndecoder.decoders.0.norm1\ndecoder.decoders.0.norm2\ndecoder.decoders.0.norm3\ndecoder.decoders.0.dropout\ndecoder.decoders.1\ndecoder.decoders.1.self_attn\ndecoder.decoders.1.self_attn.linear_q\ndecoder.decoders.1.self_attn.linear_k\ndecoder.decoders.1.self_attn.linear_v\ndecoder.decoders.1.self_attn.linear_out\ndecoder.decoders.1.self_attn.dropout\ndecoder.decoders.1.src_attn\ndecoder.decoders.1.src_attn.linear_q\ndecoder.decoders.1.src_attn.linear_k\ndecoder.decoders.1.src_attn.linear_v\ndecoder.decoders.1.src_attn.linear_out\ndecoder.decoders.1.src_attn.dropout\ndecoder.decoders.1.feed_forward\ndecoder.decoders.1.feed_forward.w_1\ndecoder.decoders.1.feed_forward.w_2\ndecoder.decoders.1.feed_forward.dropout\ndecoder.decoders.1.norm1\ndecoder.decoders.1.norm2\ndecoder.decoders.1.norm3\ndecoder.decoders.1.dropout\ndecoder.decoders.2\ndecoder.decoders.2.self_attn\ndecoder.decoders.2.self_attn.linear_q\ndecoder.decoders.2.self_attn.linear_k\ndecoder.decoders.2.self_attn.linear_v\ndecoder.decoders.2.self_attn.linear_out\ndecoder.decoders.2.self_attn.dropout\ndecoder.decoders.2.src_attn\ndecoder.decoders.2.src_attn.linear_q\ndecoder.decoders.2.src_attn.linear_k\ndecoder.decoders.2.src_attn.linear_v\ndecoder.decoders.2.src_attn.linear_out\ndecoder.decoders.2.src_attn.dropout\ndecoder.decoders.2.feed_forward\ndecoder.decoders.2.feed_forward.w_1\ndecoder.decoders.2.feed_forward.w_2\ndecoder.decoders.2.feed_forward.dropout\ndecoder.decoders.2.norm1\ndecoder.decoders.2.norm2\ndecoder.decoders.2.norm3\ndecoder.decoders.2.dropout\ndecoder.decoders.3\ndecoder.decoders.3.self_attn\ndecoder.decoders.3.self_attn.linear_q\ndecoder.decoders.3.self_attn.linear_k\ndecoder.decoders.3.self_attn.linear_v\ndecoder.decoders.3.self_attn.linear_out\ndecoder.decoders.3.self_attn.dropout\ndecoder.decoders.3.src_attn\ndecoder.decoders.3.src_attn.linear_q\ndecoder.decoders.3.src_attn.linear_k\ndecoder.decoders.3.src_attn.linear_v\ndecoder.decoders.3.src_attn.linear_out\ndecoder.decoders.3.src_attn.dropout\ndecoder.decoders.3.feed_forward\ndecoder.decoders.3.feed_forward.w_1\ndecoder.decoders.3.feed_forward.w_2\ndecoder.decoders.3.feed_forward.dropout\ndecoder.decoders.3.norm1\ndecoder.decoders.3.norm2\ndecoder.decoders.3.norm3\ndecoder.decoders.3.dropout\ndecoder.decoders.4\ndecoder.decoders.4.self_attn\ndecoder.decoders.4.self_attn.linear_q\ndecoder.decoders.4.self_attn.linear_k\ndecoder.decoders.4.self_attn.linear_v\ndecoder.decoders.4.self_attn.linear_out\ndecoder.decoders.4.self_attn.dropout\ndecoder.decoders.4.src_attn\ndecoder.decoders.4.src_attn.linear_q\ndecoder.decoders.4.src_attn.linear_k\ndecoder.decoders.4.src_attn.linear_v\ndecoder.decoders.4.src_attn.linear_out\ndecoder.decoders.4.src_attn.dropout\ndecoder.decoders.4.feed_forward\ndecoder.decoders.4.feed_forward.w_1\ndecoder.decoders.4.feed_forward.w_2\ndecoder.decoders.4.feed_forward.dropout\ndecoder.decoders.4.norm1\ndecoder.decoders.4.norm2\ndecoder.decoders.4.norm3\ndecoder.decoders.4.dropout\ndecoder.decoders.5\ndecoder.decoders.5.self_attn\ndecoder.decoders.5.self_attn.linear_q\ndecoder.decoders.5.self_attn.linear_k\ndecoder.decoders.5.self_attn.linear_v\ndecoder.decoders.5.self_attn.linear_out\ndecoder.decoders.5.self_attn.dropout\ndecoder.decoders.5.src_attn\ndecoder.decoders.5.src_attn.linear_q\ndecoder.decoders.5.src_attn.linear_k\ndecoder.decoders.5.src_attn.linear_v\ndecoder.decoders.5.src_attn.linear_out\ndecoder.decoders.5.src_attn.dropout\ndecoder.decoders.5.feed_forward\ndecoder.decoders.5.feed_forward.w_1\ndecoder.decoders.5.feed_forward.w_2\ndecoder.decoders.5.feed_forward.dropout\ndecoder.decoders.5.norm1\ndecoder.decoders.5.norm2\ndecoder.decoders.5.norm3\ndecoder.decoders.5.dropout\nctc\nctc.ctc_lo\nctc.ctc_loss\ncriterion_att\ncriterion_att.criterion\n"
     ]
    }
   ],
   "source": [
    "for n, m in net.named_modules():\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = []\n",
    "def hook(module, input, output):\n",
    "    print(\"This is input\")\n",
    "    encoder_input.append(input)\n",
    "\n",
    "for n, m in net.named_modules():\n",
    "    if n == 'encoder.encoders.0':\n",
    "        m.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_PATH = \"/home/jmpark/espnet/egs2/librispeech/asr1/data/dev_clean\"\n",
    "WAV_LIST_PATH = TEST_DATA_PATH + \"/wav.scp\"\n",
    "\n",
    "file_name_list = []\n",
    "\n",
    "with open(WAV_LIST_PATH, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        num, name = line.split(' ')\n",
    "        file_name_list.append(name[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_num = 5\n",
    "speech, rate = soundfile.read(file_name_list[audio_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is input\n"
     ]
    }
   ],
   "source": [
    "out = speech2text(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(144160,)\ntorch.Size([1, 187, 512])\n"
     ]
    }
   ],
   "source": [
    "print(speech.shape)\n",
    "print(encoder_input[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape [1, seqlen, d_model]\n"
   ]
  }
 ]
}