{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('espnet': conda)",
   "metadata": {
    "interpreter": {
     "hash": "fe19ea1dd1c022a555d1887e1747d5402a0438a9e4c1a077d5b483c4b395fede"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention layer.\n",
    "\n",
    "    Args:\n",
    "        n_head (int): The number of heads.\n",
    "        n_feat (int): The number of features.\n",
    "        dropout_rate (float): Dropout rate.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_head, n_feat, dropout_rate):\n",
    "        \"\"\"Construct an MultiHeadedAttention object.\"\"\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert n_feat % n_head == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = n_feat // n_head\n",
    "        self.h = n_head\n",
    "        self.linear_q = nn.Linear(n_feat, n_feat)\n",
    "        self.linear_k = nn.Linear(n_feat, n_feat)\n",
    "        self.linear_v = nn.Linear(n_feat, n_feat)\n",
    "        self.linear_out = nn.Linear(n_feat, n_feat)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward_qkv(self, query, key, value):\n",
    "        \"\"\"Transform query, key and value.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): Query tensor (#batch, time1, size).\n",
    "            key (torch.Tensor): Key tensor (#batch, time2, size).\n",
    "            value (torch.Tensor): Value tensor (#batch, time2, size).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Transformed query tensor (#batch, n_head, time1, d_k).\n",
    "            torch.Tensor: Transformed key tensor (#batch, n_head, time2, d_k).\n",
    "            torch.Tensor: Transformed value tensor (#batch, n_head, time2, d_k).\n",
    "\n",
    "        \"\"\"\n",
    "        n_batch = query.size(0)\n",
    "        q = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)\n",
    "        k = self.linear_k(key).view(n_batch, -1, self.h, self.d_k)\n",
    "        v = self.linear_v(value).view(n_batch, -1, self.h, self.d_k)\n",
    "        q = q.transpose(1, 2)  # (batch, head, time1, d_k)\n",
    "        k = k.transpose(1, 2)  # (batch, head, time2, d_k)\n",
    "        v = v.transpose(1, 2)  # (batch, head, time2, d_k)\n",
    "\n",
    "        return q, k, v\n",
    "\n",
    "    def forward_attention(self, value, scores, mask, survive_head_idx=[-1]):\n",
    "        \"\"\"Compute attention context vector.\n",
    "\n",
    "        Args:\n",
    "            value (torch.Tensor): Transformed value (#batch, n_head, time2, d_k).\n",
    "            scores (torch.Tensor): Attention score (#batch, n_head, time1, time2).\n",
    "            mask (torch.Tensor): Mask (#batch, 1, time2) or (#batch, time1, time2).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Transformed value (#batch, time1, d_model)\n",
    "                weighted by the attention score (#batch, time1, time2).\n",
    "\n",
    "        \"\"\"\n",
    "        n_batch = value.size(0)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).eq(0)  # (batch, 1, *, time2)\n",
    "            min_value = float(\n",
    "                numpy.finfo(torch.tensor(0, dtype=scores.dtype).numpy().dtype).min\n",
    "            )\n",
    "            scores = scores.masked_fill(mask, min_value)\n",
    "            self.attn = torch.softmax(scores, dim=-1).masked_fill(\n",
    "                mask, 0.0\n",
    "            )  # (batch, head, time1, time2)\n",
    "        else:\n",
    "            self.attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)\n",
    "\n",
    "        if survive_head_idx[0] is not -1:\n",
    "            del_attn = torch.empty_like(self.attn)\n",
    "            del_attn[:,survive_head_idx,:].copy_(self.attn[:,survive_head_idx,:])\n",
    "            self.attn = del_attn\n",
    "\n",
    "        p_attn = self.dropout(self.attn)\n",
    "        x = torch.matmul(p_attn, value)  # (batch, head, time1, d_k)\n",
    "        x = (\n",
    "            x.transpose(1, 2).contiguous().view(n_batch, -1, self.h * self.d_k)\n",
    "        )  # (batch, time1, d_model)\n",
    "\n",
    "        return self.linear_out(x), self.attn  # (batch, time1, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask, survive_head_idx=[-1]):\n",
    "        \"\"\"Compute scaled dot product attention.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): Query tensor (#batch, time1, size).\n",
    "            key (torch.Tensor): Key tensor (#batch, time2, size).\n",
    "            value (torch.Tensor): Value tensor (#batch, time2, size).\n",
    "            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or\n",
    "                (#batch, time1, time2).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor (#batch, time1, d_model).\n",
    "\n",
    "        \"\"\"\n",
    "        q, k, v = self.forward_qkv(query, key, value)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        return self.forward_attention(v, scores, mask, survive_head_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hook():\n",
    "    def __init__(self, module, layer_idx, survive_head_idx, module_type='encoder', attn_type='self_attn', backward=False):\n",
    "        if backward==False:\n",
    "            self.hook = module.register_forward_pre_hook(self.hook_fn)\n",
    "        else:\n",
    "            self.hook = module.register_backward_hook(self.hook_fn)\n",
    "\n",
    "        self.layer_idx = layer_idx\n",
    "        self.survive_head_idx = survive_head_idx\n",
    "        self.name = str(self.layer_idx) + '_' + str(self.survive_head_idx)\n",
    "\n",
    "\n",
    "    def hook_fn(self, module, input):\n",
    "        query, key, value, mask, head_idx = input\n",
    "        head_idx[0] = self.survive_head_idx\n",
    "        return query, key, value, mask, head_idx\n",
    "        \n",
    "    def close(self):\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_attn = MultiHeadedAttention(8, 512, 0.1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MultiHeadedAttention(\n  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n  (linear_out): Linear(in_features=512, out_features=512, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\n"
     ]
    }
   ],
   "source": [
    "print(test_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_q = torch.rand(1, 99, 512).cuda()\n",
    "temp_k = torch.rand(1, 99, 512).cuda()\n",
    "temp_v = torch.rand(1, 99, 512).cuda()\n",
    "temp_mask = torch.ones(1, 1, 99).bool().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_fn = Hook(module=test_attn, layer_idx=0, survive_head_idx=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_after_delete = test_attn(temp_q, temp_k, temp_v, temp_mask, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[-1]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "hook_fn.input[4]"
   ]
  }
 ]
}