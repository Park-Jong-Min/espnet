{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('speech': conda)",
   "metadata": {
    "interpreter": {
     "hash": "611231d50d89399c7812e642eac108257e44491cbc2ead4a7a526b1be52f90d6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile\n",
    "import torch\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from espnet_model_zoo.downloader import ModelDownloader\n",
    "from espnet2.bin.asr_inference import Speech2Text\n",
    "from espnet2.bin.jm_utils import *\n",
    "from zeroth_data_loader import zeroth_dataset\n",
    "from extract_grad_cam import *\n",
    "from zeroth_model_loader import load_model\n",
    "from hook_related_fn import *\n",
    "from model_dict import model_dict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_exp_name = 'asr_TF_CUSTOM_HEAD4_ENCL12_FF512_relu_lr46_ctc31'\n",
    "asr_config_file = asr_exp_name + '/config.yaml'\n",
    "asr_model_file = asr_exp_name + '/valid.acc.ave.pth'\n",
    "exp_dir = '/home/jmpark/home_data_jmpark/espnet/egs2/zeroth_korean/asr1/exp'\n",
    "\n",
    "# Set test wav for attention image extraction\n",
    "dataset = zeroth_dataset()\n",
    "\n",
    "speech2text = load_model(asr_config_file=asr_config_file, \n",
    "                            asr_model_file=asr_model_file, \n",
    "                            prune_ratio=0.0, \n",
    "                            prune_mode='no_prune',\n",
    "                            device='cpu')\n",
    "\n",
    "net = speech2text.asr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ESPnetASRModel(\n  (frontend): DefaultFrontend(\n    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)\n    (frontend): Frontend()\n    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)\n  )\n  (normalize): GlobalMVN(stats_file=exp/STATS_TF_CUSTOM_HEAD4_ENCL12_FF512_relu_lr46_ctc31/train/feats_stats.npz, norm_means=True, norm_vars=True)\n  (encoder): TransformerEncoder(\n    (embed): Conv2dSubsampling(\n      (conv): Sequential(\n        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))\n        (1): ReLU()\n        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n        (3): ReLU()\n      )\n      (out): Sequential(\n        (0): Linear(in_features=4864, out_features=256, bias=True)\n        (1): PositionalEncoding(\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (encoders): MultiSequential(\n      (0): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=256, out_features=512, bias=True)\n          (w_2): Linear(in_features=512, out_features=256, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (1): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=256, out_features=512, bias=True)\n          (w_2): Linear(in_features=512, out_features=256, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (2): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=256, out_features=512, bias=True)\n          (w_2): Linear(in_features=512, out_features=256, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (3): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=256, out_features=512, bias=True)\n          (w_2): Linear(in_features=512, out_features=256, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (4): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=256, out_features=512, bias=True)\n          (w_2): Linear(in_features=512, out_features=256, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (5): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=256, out_features=512, bias=True)\n          (w_2): Linear(in_features=512, out_features=256, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (6): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=256, out_features=512, bias=True)\n          (w_2): Linear(in_features=512, out_features=256, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (7): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=256, out_features=512, bias=True)\n          (w_2): Linear(in_features=512, out_features=256, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (8): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=256, out_features=512, bias=True)\n          (w_2): Linear(in_features=512, out_features=256, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (9): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=256, out_features=512, bias=True)\n          (w_2): Linear(in_features=512, out_features=256, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (10): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=256, out_features=512, bias=True)\n          (w_2): Linear(in_features=512, out_features=256, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (11): EncoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=256, out_features=512, bias=True)\n          (w_2): Linear(in_features=512, out_features=256, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n  )\n  (decoder): TransformerDecoder(\n    (embed): Sequential(\n      (0): Embedding(5000, 256)\n      (1): PositionalEncoding(\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n    (output_layer): Linear(in_features=256, out_features=5000, bias=True)\n    (decoders): MultiSequential(\n      (0): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=256, out_features=512, bias=True)\n          (w_2): Linear(in_features=512, out_features=256, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (1): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=256, out_features=512, bias=True)\n          (w_2): Linear(in_features=512, out_features=256, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (2): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=256, out_features=512, bias=True)\n          (w_2): Linear(in_features=512, out_features=256, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (3): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=256, out_features=512, bias=True)\n          (w_2): Linear(in_features=512, out_features=256, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (4): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=256, out_features=512, bias=True)\n          (w_2): Linear(in_features=512, out_features=256, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (5): DecoderLayer(\n        (self_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (src_attn): MultiHeadedAttention(\n          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (linear_q_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_k_pred): Linear(in_features=256, out_features=4, bias=True)\n          (linear_v_pred): Linear(in_features=256, out_features=4, bias=True)\n          (prune_act_fn): ReLU()\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=256, out_features=512, bias=True)\n          (w_2): Linear(in_features=512, out_features=256, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (ctc): CTC(\n    (ctc_lo): Linear(in_features=256, out_features=5000, bias=True)\n    (ctc_loss): CTCLoss()\n  )\n  (criterion_att): LabelSmoothingLoss(\n    (criterion): KLDivLoss()\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = []\n",
    "\n",
    "for name, module in net.named_modules():\n",
    "    if 'pred' in name:\n",
    "        module_list.append((module, name))\n",
    "\n",
    "hookF = {name: OutHook(module) for (module, name) in module_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = zeroth_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = dataset[15][1]['speech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = speech2text(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34752\n5521\n0.8411314456721916\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "total_nonzero = 0\n",
    "\n",
    "for name in hookF:\n",
    "    if 'encoder' in name:\n",
    "        output = hookF[name].output[0]\n",
    "        total += output.size(1) * output.size(2)\n",
    "        total_nonzero += torch.count_nonzero(torch.nn.functional.relu(output)).item()\n",
    "    \n",
    "    elif 'linear_q_pred' in name:\n",
    "        output = hookF[name].output\n",
    "\n",
    "        for i in range(len(output)):\n",
    "            out = output[i]\n",
    "            total += out.size(1) * out.size(2)\n",
    "            total_nonzero += torch.count_nonzero(torch.nn.functional.relu(out)).item()\n",
    "\n",
    "    else:\n",
    "        output = hookF[name].output[-1]\n",
    "        total += output.size(1) * output.size(2)\n",
    "        total_nonzero += torch.count_nonzero(torch.nn.functional.relu(output)).item()\n",
    "\n",
    "print(total)\n",
    "print(total_nonzero)\n",
    "print(1-(total_nonzero/total))\n"
   ]
  }
 ]
}